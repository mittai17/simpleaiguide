---
title: "Googleâ€™s Biggest AI Shift Yet: Titans, Miris, Lux & the Rise of Gemini"
description: "Google has been firing off updates at lightning speedâ€”but this week was different. Instead of releasing one big model, they updated almost every layer of the AI stack including Titans, Miris, and Lux."
pubDate: 2025-12-11
heroImage: "/images/blog/news/titans-memory-architecture/image-1.png"
category: 'News'
tags: ['Google', 'DeepMind', 'Titans', 'Gemini', 'AI News', 'Machine Learning']
keyTakeaways: [
  "Titans is Google's new memory architecture that learns in real-time, functioning like a human's long-term memory.",
  "Miris is a unified framework that explains and improves upon sequence models like Transformers and Mamba.",
  "Lux is a new computer-use agent that outperforms Claude and Operator in automating complex UI tasks.",
  "Gemini is now growing faster than ChatGPT, driven by Android integration and new models like Nano Banana."
]
---
import YouTubeEmbed from "../../../components/YouTubeEmbed.astro";

<YouTubeEmbed id="GGdjty9pAtg" title="Googleâ€™s Biggest AI Shift Yet: Titans, Miris, Lux" />

**Googleâ€™s Titans Just Solved AIâ€™s Biggest Weakness, But...**

## ğŸ§  Introduction: The Week Google Quietly Changed AI Forever

Google has been firing off updates at lightning speedâ€”but this week was different.
Instead of releasing one big model, they updated almost every layer of the AI stack:

- **Titans**: a breakthrough long-term memory architecture
- **Miris**: a unified theory of sequence models
- **Lux**: a new computer-use agent beating Gemini, Operator & Claude
- **Nano Banana 2 Flash**: a cheaper, near-Pro image generation model
- **AI-rewritten headlines test**: controversial but strategic
- **Gemini user growth**: now rising faster than ChatGPT

> [!IMPORTANT]
> **OpenAI responded with a Code Red internally.**

This article breaks down everythingâ€”simply, accurately, and in a way real users understand.

## ğŸ›ï¸ 1. Titans â€” Googleâ€™s Answer to Transformer Limits

![Diagram of Google's Titans Memory Architecture showing the Long-Term Memory module](/images/blog/news/titans-memory-architecture/image-1.png)

Transformers are amazingâ€”but they fail on truly long contexts.
Computation explodes. Memory collapses. Accuracy drops.

**Titans fixes this with a dual-memory architecture:**

### ğŸ¯ Short-Term Memory â†’ Windowed Attention

Precise. High-resolution. Perfect for recent context.

### ğŸ§  Long-Term Memory â†’ Updated Continuously

This is the breakthrough. Unlike normal LLMs:

- It learns during use
- Stores only â€œsurprisingâ€ information
- Forgets intelligently
- Adapts its memory while generating

### ğŸ“Œ Why Titans Is Important

Because todayâ€™s LLMs donâ€™t learn on the fly.

Titans might be the first architecture that acts like a human with:

- Working memory (short-term)
- Life-long memory (long-term)

### ğŸ“Š Titansâ€™ Benchmark Dominance

Titans crushed:

- GPT-4 Recurrent
- Gemma 9B
- Llama 3.1 70B
- Even Llama paired with retrieval tools

Its memory-as-context model handled over 2M tokens.

This is the beginning of post-Transformer AI.

## ğŸ§¬ 2. Miris â€” Googleâ€™s Unified Theory of Memory Models

![The Miris Framework architecture illustrating how it manages temporal abstraction](/images/blog/news/titans-memory-architecture/image-2.png)

Miris is not a single modelâ€”it's a framework explaining:

- Transformers
- Mamba
- RWKV
- RetNet
- DeltaNet

All are just different answers to four questions:

1. How is memory shaped?
2. What gets stored?
3. How fast does memory decay?
4. How does memory update?

This framework produced three new families:

- ğŸ”¹ Moneta
- ğŸ”¹ Yad
- ğŸ”¹ Mamora

Some of these outperform Mamba-2 in ultra-long tasks.

### ğŸ’ The Big Idea

Miris gives Google a roadmap to build:

- Models that learn continuously
- Models that reason like teenagers (Iliaâ€™s words)
- Models that donâ€™t depend on enormous static datasets

**This is huge.**

## ğŸ–¥ï¸ 3. Lux â€” The Computer-Use Agent That Just Changed Everything

![Visualization of the Lux Agent's reasoning workflow](/images/blog/news/titans-memory-architecture/image-3.png)

While Google was overhauling architecture, the Open AGI Foundation dropped something shocking:

### âš¡ Lux: The New King of Computer-Use Agents

Lux isnâ€™t a chatbot.
Itâ€™s not a browser automation plugin.
Itâ€™s not â€œOperator lite.â€

**Lux uses a computer like a human:**

- Reads the screen
- Clicks
- Scrolls
- Types
- Follows UI elements
- Performs tasks across OS, browser, spreadsheets, emails

### ğŸ“ˆ Benchmarks: Lux vs the World

A massive gap.

### ğŸ® Why It Wins

Lux has 3 modes:

1. **Actor Mode** â€“ fast steps (1 second/step)
2. **Thinker Mode** â€“ plans multi-step tasks
3. **Tasker Mode** â€“ executes deterministic instructions with retries

### ğŸ§ª Agentic Active Pre-Training

**Lux learns by acting, not reading logs.**

OS-Gym can generate:

- 1000+ OS replicas
- 1400 interaction trajectories per minute

This gives Lux intuition about computersâ€”something NO model had before.

## ğŸŒ 4. Nano Banana 2 Flash â€” Same Pro Quality, Lower Cost

Google is preparing to launch:

**Nano Banana 2 Flash**
â€” an ultra-cheap, ultra-fast version of the Nano Banana 2 Pro image model.

Early signals show:

- Nearly Pro-level performance
- Much lower compute cost
- Ideal for high-frequency workloads

Nano Banana is powering Geminiâ€™s growth, especially among younger users using it for:

- Aesthetic images
- Daily creativity
- Quick edits and remixes

**Flash will supercharge this further.**

## ğŸ“° 5. Google Is Quietly Rewriting Headlines Using AI (And Itâ€™s Causing Chaos)

Users noticed Discover feed headlines being rewritten:

- Sometimes oversimplified
- Sometimes misleading
- Sometimes completely wrong

Examples include:

- â€œPlayers clone in-game childrenâ€ â†’ rewritten as â€œBG3 players exploit childrenâ€
- â€œPrice not announcedâ€ â†’ rewritten as â€œPrice revealedâ€

> [!CAUTION]
> **Newsrooms are furious. Users are confused.**

Google calls it a â€œsmall UI experiment.â€

But the real story is: Google is testing AI-optimized headlines for user engagement.

Expect this to evolve into:

- AI-driven summarization
- Dynamic rewriting based on user interests
- Personalized titles

News publishers wonâ€™t love it.

## ğŸ“Š 6. Gemini Is Growing Faster Than ChatGPT â€” Here Are the Real Numbers

![Chart showing the exponential growth of Gemini's context window capabilities](/images/blog/news/titans-memory-architecture/image-4.png)

[Sensor Towerâ€™s new report](https://sensortower.com/) shows the shift clearly:

### ğŸ“¥ Downloads (YoY)

- **Gemini**: +190%
- **Claude**: +190%
- **Perplexity**: +215%
- **ChatGPT**: +85%

For a deeper dive into how ChatGPT compares to other models, check out our analysis on [Stop Using ChatGPT?](/blog/comparisons/stop-using-chatgpt-2025).

### ğŸ‘¥ Monthly Active Users (MAUs)

- ChatGPT â†’ 55% share
- Gemini â†’ 30% growth in 3 months
- ChatGPT â†’ only 6% growth

### ğŸ“± Engagement Time

- Gemini: 11 minutes/day (+120% since March)
- ChatGPT: +6%

### Why?

Because of:

- Nano Banana model
- Gemini built into Android
- Default ecosystem advantage

In India, Geminiâ€™s adoption is exploding because of Android dominance. Make sure you are using the best prompts with our [Master Prompt Engineering Guide](/blog/prompt-engineering/master-prompt-engineering-2025).

## ğŸ”¥ 7. OpenAIâ€™s â€œCode Redâ€ & the New Garlic Model

After seeing Geminiâ€™s acceleration, Sam Altman issued a code red:

- Pause advertising experiments
- Focus entirely on core product upgrades
- Accelerate personalization
- Improve reliability
- Improve reasoning
- Improve image generation

And most importantly:

### ğŸ§„ OpenAI is building a new model codenamed Garlic

Insiders say:

- Designed to beat Gemini 3
- Focus on coding & reasoning
- Expected release Q1â€“Q2 2025

**The race is officially back on.**

## ğŸ§© Conclusion: 2025 Is Becoming the Most Important Year in AI History

This week alone:

- Google rewrote the rules of model architecture
- A new agent (Lux) beat everyone
- Gemini began outgrowing ChatGPT
- OpenAI entered emergency mode
- Long-term memory systems reached real-world feasibility

The AI landscape is shifting faster than everâ€”and the winners will be those who can adapt. This reinforces the broader [Agentic Shift](/blog/best-ai-tools/the-agentic-shift) we are seeing across the industry.

---

## â“ Frequently Asked Questions (FAQ)

### What is Google's Titans Memory?
Titans is a new neural memory architecture from Google DeepMind that introduces a persistent long-term memory module. Unlike standard Transformers that forget context once it leaves the context window, Titans can "learn" and store surprising information indefinitely, similar to how human memory works.

### How does the Miris framework work?
Miris is a "User Guide" for sequence models. It provides a mathematical framework that unifies different architectures like Transformers, Mamba, and RWKV. By answering four key questionsâ€”Memory Shape, Information Storage, Memory Decay, and Update Mechanismsâ€”it allows researchers to build better, more efficient models that learn continuously.

### Is Lux better than Claude for computer use?
Yes, in recent benchmarks, the **Lux** agent significantly outperformed standard computer-use models like Claude. Lux uses "Agentic Active Pre-Training," meaning it learned by practicing on thousands of simulated operating systems rather than just watching videos, giving it a deeper intuition for how UIs respond to clicks and typing.

### When will Gemini 3 be released?
While no official date is set, the release of the "Garlic" codenamed project from OpenAI suggests a competitive release window in Q1â€“Q2 2025. Google's rapid updates to the entire stack (Titans, Miris, Lux) indicate Gemini 3's infrastructure is already being deployed.
